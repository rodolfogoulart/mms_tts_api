# ğŸ™ï¸ Hebrew & Greek TTS API (Sherpa-ONNX)

API de Text-to-Speech para **Hebraico, Grego e PortuguÃªs** usando **Sherpa-ONNX** com modelos MMS-TTS!

## ğŸš€ **VersÃ£o Sherpa-ONNX - Ãudio Perfeito**

âœ¨ **Principais caracterÃ­sticas:**
- ğŸµ **Qualidade perfeita**: Usa Sherpa-ONNX para prÃ©-processamento correto
- ğŸ”¥ **Ultra-leve**: Docker image ~500MB (vs ~2.5GB PyTorch)
- âš¡ **RÃ¡pido**: InferÃªncia otimizada com ONNX
- ğŸ’¾ **Eficiente**: ~50-100MB de memÃ³ria
- ğŸŒ **MultilÃ­ngue**: Hebraico, Grego e PortuguÃªs
- ğŸ¯ **FÃ¡cil**: API simples com cache inteligente

## ğŸ”§ **Por Que Sherpa-ONNX?**

Os modelos MMS-TTS do repositÃ³rio `willwade/mms-tts-multilingual-models-onnx` foram **convertidos especificamente para Sherpa-ONNX**. Usar ONNX Runtime diretamente resulta em Ã¡udio com som de "vento" porque falta o prÃ©-processamento essencial:
- âŒ **ONNX Runtime direto**: Som de vento/sopro (inÃºtil)
- âœ… **Sherpa-ONNX**: Fala clara e inteligÃ­vel (perfeito!)

Sherpa-ONNX aplica automaticamente:
- ConversÃ£o de caracteres para phonemes
- InserÃ§Ã£o de blank tokens
- NormalizaÃ§Ã£o de texto
- Processamento correto de diacrÃ­ticos (niqqud, acentos)

## âœ¨ **Novidade: Word-Level Alignment** ğŸ¯

Suporte a **sincronizaÃ§Ã£o palavra-por-palavra**!

- ğŸ¤ Endpoint `/speak_sync` retorna timestamps por palavra
- ğŸ¨ Perfeito para karaoke-style highlighting
- ğŸ“– Ideal para aplicativos de aprendizado de idiomas
- ğŸ”¤ Preserva Unicode (niqqud hebraico, acentos gregos)

**DocumentaÃ§Ã£o completa**: [`resources/WORD_ALIGNMENT_GUIDE.md`](resources/WORD_ALIGNMENT_GUIDE.md)

---

## ğŸŒŸ **Modelos Suportados**

### 1. **MMS-TTS Hebrew (Sherpa-ONNX)** 
- âœ… **Hebraico nativo** (`heb`)
- ğŸ¯ Modelo otimizado para hebraico bÃ­blico e moderno
- ğŸ“œ Suporte completo a niqqud (pontos vocÃ¡licos)
- ğŸš€ Tamanho: ~10-15MB
- âš¡ Fala clara e natural

### 2. **MMS-TTS Greek (Sherpa-ONNX)**
- âœ… **Grego nativo** (`ell`) 
- ğŸ›ï¸ Modelo ONNX especializado para grego
- ğŸ“œ Suporte completo a caracteres gregos
- ğŸš€ Performance extrema (~10-15MB)
- âš¡ InferÃªncia 3-5x mais rÃ¡pida que PyTorch

### 3. **MMS-TTS Portuguese ONNX**
- âœ… **PortuguÃªs nativo** (`por`)
- ğŸ‡§ğŸ‡· Modelo ONNX especializado para portuguÃªs
- ğŸš€ Performance extrema (~10-15MB)
- âš¡ InferÃªncia 3-5x mais rÃ¡pida que PyTorch

**Fonte dos modelos**: [`willwade/mms-tts-multilingual-models-onnx`](https://huggingface.co/willwade/mms-tts-multilingual-models-onnx)

## ğŸš€ **InÃ­cio RÃ¡pido**

### Docker Compose (Recomendado)

#### **ğŸ–¥ï¸ Desenvolvimento Local com GPU NVIDIA**
Para rodar no seu notebook/desktop com GPU NVIDIA:

```bash
# PrÃ©-requisito: NVIDIA Container Toolkit instalado
# Verificar GPU disponÃ­vel
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# Build e iniciar com GPU CUDA
docker-compose -f docker-compose.local.yml up --build

# Rodar em background
docker-compose -f docker-compose.local.yml up -d

# Ver logs em tempo real
docker-compose -f docker-compose.local.yml logs -f

# Parar
docker-compose -f docker-compose.local.yml down

# Testar
curl http://localhost:8000/health
```

**ConfiguraÃ§Ãµes GPU (docker-compose.local.yml):**
- âœ… Whisper model: `medium` (~1.5GB VRAM, 90-98% acurÃ¡cia)
- âœ… Device: `cuda` (GPU NVIDIA)
- âœ… Compute type: `float16` (otimizado para GPU)
- âœ… Fallback automÃ¡tico para CPU caso CUDA nÃ£o disponÃ­vel

#### **â˜ï¸ ProduÃ§Ã£o VPS/Cloud (CPU)**
Para rodar em VPS sem GPU (ex: Oracle Cloud):

```bash
# Build e iniciar (CPU-only)
docker-compose up --build

# Rodar em background
docker-compose up -d

# Testar
curl http://localhost:8000/health
```

**ConfiguraÃ§Ãµes CPU (Dockerfile.coolify):**
- âœ… Whisper model: `small` (~500MB, 85-95% acurÃ¡cia)
- âœ… Device: `cpu` (ARM64 otimizado)
- âœ… Compute type: `int8` (quantizaÃ§Ã£o para economia de memÃ³ria)

### Docker Manual

```bash
# Build da imagem
docker build -t hebrew-greek-tts .

# Executar API
docker run -p 8000:8000 hebrew-greek-tts

# Testar
curl http://localhost:8000/health
```

### ExecuÃ§Ã£o Local

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Executar API
python -m uvicorn app.multi_model_api:app --host 0.0.0.0 --port 8000

# DocumentaÃ§Ã£o interativa
open http://localhost:8000/docs
```

## ğŸ“š **Exemplos de Uso**

### Hebraico
```bash
curl -X POST "http://localhost:8000/speak" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "text=×©×œ×•× ×¢×•×œ×, ××™×š ××ª×” ×”×™×•×?&lang=heb&model=hebrew" \
     --output hebrew_audio.mp3
```

### Grego  
```bash
curl -X POST "http://localhost:8000/speak" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "text=Î“ÎµÎ¹Î± ÏƒÎ±Ï‚, Ï€ÏÏ‚ ÎµÎ¯ÏƒÏ„Îµ ÏƒÎ®Î¼ÎµÏÎ±;&lang=ell&model=greek" \
     --output greek_audio.mp3
```

### Auto-detecÃ§Ã£o de Modelo
```bash
curl -X POST "http://localhost:8000/speak" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "text=×©×œ×•× ×¢×•×œ×&lang=heb" \
     --output auto_hebrew.mp3
```

## ğŸŒ **Idiomas Suportados**

| CÃ³digo | Idioma | Modelo | Exemplo |
|--------|--------|--------|---------|
| `heb` | Hebraico | MMS-TTS Hebrew | ×©×œ×•× ×¢×•×œ× |
| `ell` | Grego | MMS-TTS Greek | Î“ÎµÎ¹Î± ÏƒÎ±Ï‚ |

## ğŸ“‹ **Endpoints da API**

### `POST /speak` - Gerar Ãudio
**ParÃ¢metros:**
- `text`: Texto para converter
- `lang`: CÃ³digo do idioma (`heb` para hebraico, `ell` para grego)

### `GET /models` - Listar Modelos
```json
{
  "models": {
    "hebrew": {"name": "MMS-TTS Hebrew", "supported_languages": {"heb": "Hebrew"}},
    "greek": {"name": "MMS-TTS Greek", "supported_languages": {"ell": "Greek"}}
  }
}
```

### `GET /languages` - Listar Idiomas
```json
{
  "supported_languages": {
    "heb": {"name": "Hebrew", "model": "MMS-TTS Hebrew"},
    "ell": {"name": "Greek", "model": "MMS-TTS Greek"}
  },
  "total_languages": 2
}
```

### `GET /health` - Status da API
```json
{
  "status": "ok",
  "device": "cpu",
  "loaded_models": ["hebrew", "greek"]
}
```

## ğŸ§ª **Testes**

### Teste Automatizado
```bash
python test_hebrew_greek.py
```

### Teste Manual - Hebraico
```bash
python -c "
import requests
response = requests.post('http://localhost:8000/speak', 
    data={'text': '×©×œ×•× ×¢×•×œ×', 'lang': 'heb'})
with open('test_hebrew.mp3', 'wb') as f:
    f.write(response.content)
print('âœ… Ãudio em hebraico gerado: test_hebrew.mp3')
"
```

### Teste Manual - Grego
```bash
python -c "
import requests  
response = requests.post('http://localhost:8000/speak',
    data={'text': 'Î“ÎµÎ¹Î± ÏƒÎ±Ï‚', 'lang': 'ell'})
with open('test_greek.mp3', 'wb') as f:
    f.write(response.content)
print('âœ… Ãudio em grego gerado: test_greek.mp3')
"
```

## ğŸ”§ **ConfiguraÃ§Ã£o AvanÃ§ada**

### VariÃ¡veis de Ambiente

#### **Para Docker Compose Local (ONNX + GPU):**
Edite `docker-compose.local.yml`:
```yaml
environment:
  # ONNX Runtime (TTS)
  - ORT_TENSORRT_FP16_ENABLE=0        # Desabilitar TensorRT FP16
  - ORT_TENSORRT_ENGINE_CACHE_ENABLE=1 # Cache de engines
  
  # Whisper (Word Alignment)
  - WHISPER_DEVICE=cuda          # Usar GPU NVIDIA
  - WHISPER_COMPUTE_TYPE=float16 # Otimizado para GPU
  - WHISPER_MODEL=medium         # Alta acurÃ¡cia (~1.5GB VRAM)
  - LOG_LEVEL=info               # debug, info, warning, error
```

#### **Para Docker Compose ProduÃ§Ã£o (ONNX CPU):**
Use o `Dockerfile.coolify` com variÃ¡veis jÃ¡ configuradas:
```bash
export ORT_TENSORRT_FP16_ENABLE=0  # ONNX Runtime otimizaÃ§Ãµes
export HF_HOME=/path/to/cache      # Cache dos modelos
```

#### **ConfiguraÃ§Ãµes Whisper:**
| VariÃ¡vel | Valores | DescriÃ§Ã£o |
|----------|---------|-----------|
| `WHISPER_MODEL` | `tiny`, `base`, `small`, `medium`, `large` | Tamanho do modelo |
| `WHISPER_DEVICE` | `cpu`, `cuda` | Dispositivo de processamento |
| `WHISPER_COMPUTE_TYPE` | `int8`, `float16`, `float32` | Tipo de computaÃ§Ã£o |

### Modelos em Cache
Os modelos ONNX sÃ£o baixados automaticamente na primeira execuÃ§Ã£o:
- `willwade/mms-tts-multilingual-models-onnx/heb` (~10-15MB)
- `willwade/mms-tts-multilingual-models-onnx/ell` (~10-15MB)
- `willwade/mms-tts-multilingual-models-onnx/por` (~10-15MB)
- `faster-whisper` (small: ~500MB, medium: ~1.5GB)

**Total para 3 idiomas**: ~30-45MB (vs ~108MB PyTorch) ğŸ‰

## ğŸ› ï¸ **SoluÃ§Ã£o de Problemas**

### GPU NVIDIA nÃ£o detectada (ONNX Runtime)
```bash
# 1. Verificar ONNX Runtime providers
python -c "import onnxruntime as ort; print(ort.get_available_providers())"
# Deve mostrar: ['CUDAExecutionProvider', 'CPUExecutionProvider']

# 2. Se CUDA nÃ£o aparecer, instalar onnxruntime-gpu
pip uninstall onnxruntime
pip install onnxruntime-gpu

# 3. Verificar CUDA no host
nvidia-smi

# 4. Fallback automÃ¡tico: Se GPU nÃ£o disponÃ­vel, usa CPU automaticamente
```

### Whisper GPU (Docker Compose Local)
```bash
# 1. Verificar NVIDIA Container Toolkit
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# 2. Se nÃ£o instalado, instalar NVIDIA Container Toolkit
# Ubuntu/Debian:
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### Erro: "python-multipart" 
```bash
docker build --no-cache -t hebrew-greek-tts .
```

### Modelos nÃ£o carregam
```bash
# Verificar espaÃ§o em disco
df -h

# Limpar cache
rm -rf ~/.cache/huggingface/

# Limpar volumes Docker
docker-compose -f docker-compose.local.yml down -v
```

### Whisper model muito lento
```bash
# Reduzir tamanho do modelo
# Edite docker-compose.local.yml:
environment:
  - WHISPER_MODEL=small  # Ao invÃ©s de medium
  - WHISPER_COMPUTE_TYPE=int8  # Ao invÃ©s de float16
```

## ğŸ“Š **Performance (ONNX)**

| Modelo | Idioma | Tamanho | Tempo/Frase (CPU) | Tempo/Frase (GPU) | Qualidade |
|--------|--------|---------|-------------------|-------------------|----------|
| MMS-TTS Hebrew ONNX | Hebraico | 10-15MB | ~0.5-1s | ~0.2-0.4s | â­â­â­â­â­ |
| MMS-TTS Greek ONNX | Grego | 10-15MB | ~0.5-1s | ~0.2-0.4s | â­â­â­â­â­ |
| MMS-TTS Portuguese ONNX | PortuguÃªs | 10-15MB | ~0.5-1s | ~0.2-0.4s | â­â­â­â­â­ |

**Nota**: Tempos 2-5x mais rÃ¡pidos que versÃ£o PyTorch anterior!

## ï¿½ **Links Ãšteis**

- ğŸ“– [DocumentaÃ§Ã£o MMS](https://arxiv.org/abs/2305.13516)
- ğŸ¤— [MMS-TTS ONNX Models](https://huggingface.co/willwade/mms-tts-multilingual-models-onnx)
- ğŸ¤— [MMS-TTS Hebrew Original](https://huggingface.co/facebook/mms-tts-heb)
- ğŸ¤— [MMS-TTS Greek Original](https://huggingface.co/facebook/mms-tts-ell)
- ğŸ³ [Docker Hub](https://hub.docker.com/)
- âš¡ [ONNX Runtime](https://onnxruntime.ai/)

## ğŸ“„ **LicenÃ§a**

- **MMS-TTS Hebrew/Greek**: CC-BY-NC 4.0
- **Este projeto**: MIT

---